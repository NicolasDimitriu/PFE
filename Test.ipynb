{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de : Notifications/PasVice/Notification4Vice.pdf\n",
      "Traitement de : Notifications/PasVice/Notification6Vice.pdf\n",
      "Traitement de : Notifications/PasVice/Notification5.pdf\n",
      "Traitement de : Notifications/PasVice/Notification1.pdf\n",
      "Traitement de : Notifications/PasVice/Notification3Vice.pdf\n",
      "Traitement de : Notifications/PasVice/Notification2.pdf\n",
      "Annotations exportées dans : Notifications/annotations_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extrait le texte d'un fichier PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def is_holiday_or_weekend(date_str):\n",
    "    \"\"\"\n",
    "    Vérifie si une date donnée tombe un jour férié ou un week-end.\n",
    "    Exemple simple à adapter selon le contexte.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date = datetime.strptime(date_str, \"%d %B %Y\")  # Exemple : \"15 février 2025\"\n",
    "        # Vérifier les week-ends\n",
    "        if date.weekday() >= 5:  # 5 = samedi, 6 = dimanche\n",
    "            return True\n",
    "\n",
    "        # Vérifier les jours fériés (ajoutez vos propres dates fériées ici)\n",
    "        holidays = [\n",
    "            \"01 January 2025\", \"14 July 2025\", \"25 December 2025\"\n",
    "        ]\n",
    "        if date.strftime(\"%d %B %Y\") in holidays:\n",
    "            return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def detect_vices(text):\n",
    "    \"\"\"\n",
    "    Détecte les vices de procédure basés sur des règles simples.\n",
    "    Retourne une liste vide si aucun vice n'est détecté.\n",
    "    \"\"\"\n",
    "    vices = []\n",
    "\n",
    "    # Absence de signature\n",
    "    if not re.search(r\"(signature|signé par)\", text, re.IGNORECASE):\n",
    "        vices.append(\"Absence de signature : La notification ne comporte pas la signature de l’huissier.\")\n",
    "\n",
    "    # Erreur de date\n",
    "    date_match = re.search(r\"\\d{1,2} \\w+ \\d{4}\", text)\n",
    "    if date_match:\n",
    "        date_str = date_match.group(0)\n",
    "        if is_holiday_or_weekend(date_str):\n",
    "            vices.append(f\"Erreur de date : La date mentionnée ({date_str}) tombe un jour férié ou non ouvré.\")\n",
    "\n",
    "    # Absence de preuve de notification\n",
    "    if not re.search(r\"(accusé de réception|preuve de remise|huissier)\", text, re.IGNORECASE):\n",
    "        vices.append(\"Absence de preuve de notification : Le courrier ne mentionne pas si une preuve de remise sera utilisée.\")\n",
    "\n",
    "    return vices  # Si la liste est vide, aucun vice n'a été détecté\n",
    "\n",
    "\n",
    "def annotate_text(text, vices):\n",
    "    \"\"\"\n",
    "    Pré-anote le texte avec les informations importantes et les vices détectés.\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "\n",
    "    # Extractions simples (dates, parties, numéro de dossier)\n",
    "    date_match = re.search(r\"\\d{1,2} \\w+ \\d{4}\", text)\n",
    "    if date_match:\n",
    "        annotations.append({\"start\": date_match.start(), \"end\": date_match.end(), \"label\": \"DATE_EMISSION\"})\n",
    "\n",
    "    party_match = re.search(r\"(?<=Partie concernée : ).+\", text)\n",
    "    if party_match:\n",
    "        annotations.append({\"start\": party_match.start(), \"end\": party_match.end(), \"label\": \"PARTIE_CONCERNEE\"})\n",
    "\n",
    "    dossier_match = re.search(r\"(?<=Numéro de dossier : )\\d+\", text)\n",
    "    if dossier_match:\n",
    "        annotations.append({\"start\": dossier_match.start(), \"end\": dossier_match.end(), \"label\": \"NUMERO_DOSSIER\"})\n",
    "\n",
    "    # Ajout des vices détectés\n",
    "    for vice in vices:\n",
    "        annotations.append({\"label\": \"VICE_NOTIFICATION\", \"description\": vice})\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def process_pdfs_in_directory(directory_path, output_json):\n",
    "    \"\"\"\n",
    "    Traite tous les fichiers PDF dans un répertoire et exporte les annotations en JSON.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Lister tous les fichiers PDF dans le répertoire\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"Traitement de : {file_path}\")\n",
    "\n",
    "            # Extraction du texte\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "\n",
    "            # Détection des vices\n",
    "            vices = detect_vices(text)\n",
    "\n",
    "            # Annotation du texte\n",
    "            annotations = annotate_text(text, vices)\n",
    "\n",
    "            # Ajouter au résultat\n",
    "            annotations = annotate_text(text, vices)\n",
    "            label = 1 if vices else 0  # 1 = avec vice, 0 = sans vice\n",
    "            results.append({\n",
    "              \"file_name\": filename,\n",
    "              \"text\": text,\n",
    "              \"annotations\": annotations,\n",
    "              \"vices_detected\": vices,\n",
    "              \"label\": label\n",
    "            })\n",
    "\n",
    "\n",
    "    # Exporter les résultats en JSON\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Annotations exportées dans : {output_json}\")\n",
    "\n",
    "# Utilisation du script\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"Notifications/PasVice\"  # Remplacez par le chemin de votre répertoire contenant les PDF\n",
    "    output_json = \"Notifications/annotations_results.json\"  # Chemin du fichier JSON de sortie\n",
    "    process_pdfs_in_directory(input_directory, output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9441a76af3254d0386d07c13ffbcedda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5146a87d19b444889c155fc463427b64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m val_dataset\u001B[38;5;241m.\u001B[39mmap(tokenize_function, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Spécifier les arguments d'entraînement\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m training_args \u001B[38;5;241m=\u001B[39m \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./results\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./logs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_best_model_at_end\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\n\u001B[1;32m     51\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# Définir un Trainer\u001B[39;00m\n\u001B[1;32m     54\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     55\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     56\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     59\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer\n\u001B[1;32m     60\u001B[0m )\n",
      "File \u001B[0;32m<string>:134\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001B[0m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1780\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1778\u001B[0m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_available():\n\u001B[0;32m-> 1780\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m   1782\u001B[0m \u001B[38;5;66;03m# Disable average tokens when using single device\u001B[39;00m\n\u001B[1;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_tokens_across_devices:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:2306\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2302\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2303\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[1;32m   2304\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2305\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m-> 2306\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/generic.py:60\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, objtype)\u001B[0m\n\u001B[1;32m     58\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 60\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:2179\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2177\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[1;32m   2178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[0;32m-> 2179\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m   2180\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2181\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccelerate>=\u001B[39m\u001B[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2182\u001B[0m         )\n\u001B[1;32m   2183\u001B[0m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[1;32m   2184\u001B[0m accelerator_state_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menabled\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_configured_state\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[0;31mImportError\u001B[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Charger les données\n",
    "def load_data(json_path):\n",
    "    import json\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Charger le fichier JSON avec vos données\n",
    "data = load_data(\"Notifications/annotations_results.json\")  # Remplacez par le chemin de votre fichier JSON\n",
    "\n",
    "# Créer un Dataset Hugging Face\n",
    "texts = [item[\"text\"] for item in data]\n",
    "labels = [item[\"label\"] for item in data]\n",
    "\n",
    "# Séparer en jeu d'entraînement et de validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Créer des datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
    "\n",
    "# Charger CamemBERT\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenizer les données\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Spécifier les arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Définir un Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "trainer.train()\n",
    "\n",
    "# Sauvegarder le modèle entraîné\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
